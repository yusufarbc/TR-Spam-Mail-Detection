{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b4014a",
   "metadata": {},
   "source": [
    "# Turkish Spam Data Set Classification with KNN\n",
    "### Developed by yusufarbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1b575",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "In this project, we're using Python3 and its data-mining modules for developing a machine learning model.\n",
    "- string module for list of punctuation.\n",
    "- csv module for reading data set.\n",
    "- numpy to allow advanced array manipulation.\n",
    "- matplotlib to draw various plots.\n",
    "- train_test_split to split the data into training and test data.\n",
    "- accuracy_score to calculate accuracy of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13c69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import string\n",
    "import csv\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15becb99",
   "metadata": {},
   "source": [
    "## The KNN Algorithm\n",
    "\n",
    "K-Nearest Neighbours (KNN) is a simple supervised learning algorithm that differs from traditional ones, like the Multinomial Naive Bayes algorithm. Unlike these, KNN doesn't have a separate training stage followed by a prediction stage. Instead, when dealing with a test data item, KNN compares its features with the features of every training data item in real time. The algorithm then selects the K nearest training data items, based on their feature similarity, and assigns the most frequent class among them to the test data item.\n",
    "\n",
    "For instance, in email classification (spam or ham), KNN compares the word frequencies of each email. The algorithm uses the Euclidean distance to measure the similarity between two emails. The closer the distance, the more alike they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4248d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KNN Algorithm\n",
    "def get_count(text):\n",
    "    wordCounts = dict()\n",
    "    for word in text.split():\n",
    "        if word in wordCounts:\n",
    "            wordCounts[word] += 1\n",
    "        else:\n",
    "            wordCounts[word] = 1\n",
    "    \n",
    "    return wordCounts\n",
    "\n",
    "def euclidean_difference(test_WordCounts, training_WordCounts):\n",
    "    total = 0\n",
    "    for word in test_WordCounts:\n",
    "        if word in test_WordCounts and word in training_WordCounts:\n",
    "            total += (test_WordCounts[word] - training_WordCounts[word])**2\n",
    "            del training_WordCounts[word]\n",
    "        else:\n",
    "            total += test_WordCounts[word]**2\n",
    "\n",
    "    for word in training_WordCounts:\n",
    "        total += training_WordCounts[word]**2\n",
    "    return total**0.5\n",
    "\n",
    "def get_class(selected_Kvalues):\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    for value in selected_Kvalues:\n",
    "        if value[0] == \"spam\":\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            ham_count += 1\n",
    "    if spam_count > ham_count:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\"\n",
    "    \n",
    "def knn_classifier(training_data, training_labels, test_data, K, tsize):\n",
    "    print(\"Running KNN Classifier...\")\n",
    "    \n",
    "    result = []\n",
    "    counter = 1\n",
    "    # word counts for training email\n",
    "    training_WordCounts = [] \n",
    "    for training_text in training_data:\n",
    "            training_WordCounts.append(get_count(training_text))\n",
    "    for test_text in test_data:\n",
    "        similarity = [] # List of euclidean distances\n",
    "        test_WordCounts = get_count(test_text)  # word counts for test email\n",
    "        # Getting euclidean difference \n",
    "        for index in range(len(training_data)):\n",
    "            euclidean_diff =\\\n",
    "                euclidean_difference(test_WordCounts, training_WordCounts[index])\n",
    "            similarity.append([training_labels[index], euclidean_diff])\n",
    "        # Sort list in ascending order based on euclidean difference\n",
    "        similarity = sorted(similarity, key = lambda i:i[1])\n",
    "        # Select K nearest neighbours\n",
    "        selected_Kvalues = [] \n",
    "        for i in range(K):\n",
    "            selected_Kvalues.append(similarity[i])\n",
    "        # Predicting the class of email\n",
    "        result.append(get_class(selected_Kvalues))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5e4f2",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The E-mail data set obtained from \"Turkish Spam V01 Data Set\". It can be found at https://archive.ics.uci.edu/ml/datasets/Turkish+Spam+V01 The data set contains 825 emails and consists of a single csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca7a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Data\n",
    "print(\"Loading data...\")\n",
    "data = []\n",
    "with open(\"trspam.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        label = str(row[-1])\n",
    "        del row[-1]\n",
    "        text = str(row)\n",
    "        \n",
    "        data.append([text, label])\n",
    "del data[0]\n",
    "del data[-1]\n",
    "data = np.array(data)\n",
    "\n",
    "# data count\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28764b77",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Data pre-processing is a critical step in data analysis and machine learning as it helps to ensure that the data is accurate, consistent, and useful for further analysis. We will clean, transform, and organize the data.\n",
    "\n",
    "punc holds a list of punctuation and symbols.\n",
    "sw holds a list of stopwords.\n",
    "\n",
    "For every record in data, for every item (symbol or punctuation) in punc, replace the item with an empty string, to delete the item from email text string.\n",
    "\n",
    "And than, iterate over list of words, and if the word is not in stopwords list, set it to lowercase, and add the word to newText. newText will contain the email but empty of stopwords. newText is assigned back to record. After every record is preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f025335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Data Pre-Processing\n",
    "print(\"Preprocessing data...\")\n",
    "punc = string.punctuation       # Punctuation list\n",
    "sw = pd.read_csv(\"stopwords-tr.txt\", encoding='utf-8')    # Stopwords list\n",
    "\n",
    "for record in data:\n",
    "        # Remove common punctuation and symbols\n",
    "        for item in punc:\n",
    "            record[0] = record[0].replace(item, \"\")\n",
    "        # Split text to words\n",
    "        splittedWords = record[0].split()\n",
    "        newText = \"\"\n",
    "        # Lowercase all letters and remove stopwords \n",
    "        for word in splittedWords:\n",
    "            if word not in sw:\n",
    "                word = word.lower()\n",
    "                newText = newText + \" \" + word      \n",
    "        record[0] = newText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1e294",
   "metadata": {},
   "source": [
    "## Determine Train and Test Set\n",
    "\n",
    "The data set splits into a training set (70%) and a testing set (30%). We use \"train_test_split\" for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc402775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n"
     ]
    }
   ],
   "source": [
    "# Splitting the Data into Training and Testing Sets\n",
    "print(\"Splitting data...\")\n",
    "features = data[:, 0]   # array containing all email text bodies\n",
    "labels = data[:, 1]     # array containing corresponding labels\n",
    "training_data, test_data, training_labels, test_labels =\\\n",
    "train_test_split(features, labels, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394ec72",
   "metadata": {},
   "source": [
    "## The KNN Model\n",
    "\n",
    "The model takes a K value. Next, it trains and tests ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8b1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Test Size\n",
    "tsize = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40cfca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare K Value\n",
    "K = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a350a174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training...\n",
      "Running KNN Classifier...\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "print(\"Model Training...\")\n",
    "result = knn_classifier(training_data, training_labels, test_data[:tsize], K, tsize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fd2541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Testing...\n"
     ]
    }
   ],
   "source": [
    "# Model Test\n",
    "print(\"Model Testing...\")\n",
    "accuracy = accuracy_score(test_labels[:tsize], result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bb2b9",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "Present the model details and test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "731f7a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size\t: 577\n",
      "test data size\t\t: 248\n",
      "K value\t\t\t: 11\n",
      "Samples tested\t\t: 248\n",
      "% accuracy\t\t: 41.935483870967744\n",
      "Number correct\t\t: 104\n",
      "Number wrong\t\t: 143\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"training data size\\t: \" + str(len(training_data)))\n",
    "print(\"test data size\\t\\t: \" + str(len(test_data)))\n",
    "print(\"K value\\t\\t\\t: \" + str(K))\n",
    "print(\"Samples tested\\t\\t: \" + str(tsize))\n",
    "print(\"% accuracy\\t\\t: \" + str(accuracy * 100))\n",
    "print(\"Number correct\\t\\t: \" + str(int(accuracy * tsize)))\n",
    "print(\"Number wrong\\t\\t: \" + str(int((1 - accuracy) * tsize)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
